{{- if .Values.blueGreen.enabled }}
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ include "noaa-oracle.fullname" . }}-bg-switchover
  labels:
    {{- include "noaa-oracle.labels" . | nindent 4 }}
  annotations:
    argocd.argoproj.io/hook: PreSync
    argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
spec:
  backoffLimit: 0
  activeDeadlineSeconds: 600
  ttlSecondsAfterFinished: 300
  template:
    metadata:
      labels:
        {{- include "noaa-oracle.selectorLabels" . | nindent 8 }}
        app.kubernetes.io/component: bg-switchover
    spec:
      serviceAccountName: {{ include "noaa-oracle.fullname" . }}-bg-hook
      restartPolicy: Never
      containers:
        - name: switchover
          image: bitnami/kubectl:latest
          command: ["/bin/bash", "-c"]
          args:
            - |
              set -euo pipefail

              NAMESPACE="{{ .Release.Namespace }}"
              SERVICE="{{ include "noaa-oracle.fullname" . }}"
              DEPLOY_PREFIX="{{ include "noaa-oracle.fullname" . }}"
              CONTAINER="{{ .Chart.Name }}"
              NEW_IMAGE="{{ .Values.image.repository }}:{{ .Values.image.tag }}"

              echo "=== Blue/Green PreSync Switchover ==="
              echo "Namespace: $NAMESPACE"
              echo "Service: $SERVICE"
              echo "New image: $NEW_IMAGE"

              # Get current active slot from Service selector
              ACTIVE=$(kubectl -n "$NAMESPACE" get svc "$SERVICE" \
                -o jsonpath='{.spec.selector.app\.kubernetes\.io/slot}' 2>/dev/null || echo "")

              if [[ -z "$ACTIVE" ]]; then
                echo "No slot selector found on Service. First deploy - initializing."
                echo "Scaling up ${DEPLOY_PREFIX}-blue..."
                kubectl -n "$NAMESPACE" scale deployment "${DEPLOY_PREFIX}-blue" --replicas=1 || true
                echo "Waiting for ${DEPLOY_PREFIX}-blue to be ready..."
                kubectl -n "$NAMESPACE" rollout status deployment "${DEPLOY_PREFIX}-blue" --timeout=300s || true
                echo "Patching Service selector to blue..."
                kubectl -n "$NAMESPACE" patch svc "$SERVICE" --type=json \
                  -p '[{"op": "add", "path": "/spec/selector/app.kubernetes.io~1slot", "value": "blue"}]' || true
                echo "Scaling down ${DEPLOY_PREFIX}-green..."
                kubectl -n "$NAMESPACE" scale deployment "${DEPLOY_PREFIX}-green" --replicas=0 || true
                echo "First deploy initialization complete."
                exit 0
              fi

              # Determine standby
              if [[ "$ACTIVE" == "blue" ]]; then STANDBY="green"; else STANDBY="blue"; fi

              ACTIVE_DEPLOY="${DEPLOY_PREFIX}-${ACTIVE}"
              STANDBY_DEPLOY="${DEPLOY_PREFIX}-${STANDBY}"

              echo "Active: $ACTIVE ($ACTIVE_DEPLOY)"
              echo "Standby: $STANDBY ($STANDBY_DEPLOY)"

              # Check if active is already running the new image
              CURRENT_IMAGE=$(kubectl -n "$NAMESPACE" get deployment "$ACTIVE_DEPLOY" \
                -o jsonpath="{.spec.template.spec.containers[?(@.name=='$CONTAINER')].image}" 2>/dev/null || echo "")
              echo "Current active image: $CURRENT_IMAGE"

              if [[ "$CURRENT_IMAGE" == "$NEW_IMAGE" ]]; then
                echo "Active deployment already running $NEW_IMAGE. No switchover needed."
                kubectl -n "$NAMESPACE" scale deployment "$STANDBY_DEPLOY" --replicas=0 2>/dev/null || true
                exit 0
              fi

              echo ""
              echo "=== Starting switchover: $ACTIVE -> $STANDBY ==="

              # Step 1: Update standby image
              echo "Step 1/7: Setting $STANDBY_DEPLOY image to $NEW_IMAGE"
              kubectl -n "$NAMESPACE" set image "deployment/$STANDBY_DEPLOY" "$CONTAINER=$NEW_IMAGE"

              # Step 2: Scale down active so Litestream flushes WAL to S3 on SIGTERM
              echo "Step 2/7: Scaling down $ACTIVE_DEPLOY (Litestream will flush WAL to S3)"
              kubectl -n "$NAMESPACE" scale deployment "$ACTIVE_DEPLOY" --replicas=0

              # Step 3: Wait for active pods to terminate
              echo "Step 3/7: Waiting for $ACTIVE_DEPLOY pods to terminate"
              kubectl -n "$NAMESPACE" wait --for=delete pod \
                -l "app.kubernetes.io/slot=$ACTIVE,app.kubernetes.io/name={{ include "noaa-oracle.name" . }}" \
                --timeout=60s 2>/dev/null || true
              echo "$ACTIVE_DEPLOY terminated, S3 backups should be up to date"

              # Step 4: Clean standby PVC so litestream-restore init container does a full restore
              echo "Step 4/7: Cleaning standby PVC (${DEPLOY_PREFIX}-${STANDBY})"
              kubectl -n "$NAMESPACE" run "pvc-cleanup-$$" --rm -i --restart=Never \
                --image=alpine:latest \
                --overrides="{
                  \"spec\": {
                    \"containers\": [{
                      \"name\": \"cleanup\",
                      \"image\": \"alpine:latest\",
                      \"command\": [\"sh\", \"-c\", \"find {{ .Values.persistence.mountPath }} -type f -name '*.db' -exec rm -fv {} + ; find {{ .Values.persistence.mountPath }} -type f -name '*.db-shm' -exec rm -fv {} + ; find {{ .Values.persistence.mountPath }} -type f -name '*.db-wal' -exec rm -fv {} + ; find {{ .Values.persistence.mountPath }} -type f -name '*.sqlite' -exec rm -fv {} + ; find {{ .Values.persistence.mountPath }} -type f -name '*.sqlite-shm' -exec rm -fv {} + ; find {{ .Values.persistence.mountPath }} -type f -name '*.sqlite-wal' -exec rm -fv {} + ; find {{ .Values.persistence.mountPath }} -type d -name '*-litestream' -exec rm -rfv {} + 2>/dev/null; echo PVC cleaned\"],
                      \"volumeMounts\": [{\"name\": \"data\", \"mountPath\": \"{{ .Values.persistence.mountPath }}\"}]
                    }],
                    \"volumes\": [{\"name\": \"data\", \"persistentVolumeClaim\": {\"claimName\": \"${DEPLOY_PREFIX}-${STANDBY}\"}}]
                  }
                }" 2>/dev/null || echo "PVC cleanup completed (or PVC was already clean)"

              # Step 5: Scale up standby (triggers Litestream restore from fresh S3 data)
              echo "Step 5/7: Scaling up $STANDBY_DEPLOY (will restore DB from S3)"
              kubectl -n "$NAMESPACE" scale deployment "$STANDBY_DEPLOY" --replicas=1

              # Step 6: Wait for readiness
              echo "Step 6/7: Waiting for $STANDBY_DEPLOY to be ready..."
              if ! kubectl -n "$NAMESPACE" rollout status deployment "$STANDBY_DEPLOY" --timeout=300s; then
                echo "ERROR: Standby deployment failed readiness check"
                echo "Rolling back: scaling down standby, scaling up previous active"
                kubectl -n "$NAMESPACE" scale deployment "$STANDBY_DEPLOY" --replicas=0
                kubectl -n "$NAMESPACE" scale deployment "$ACTIVE_DEPLOY" --replicas=1
                exit 1
              fi

              # Step 7: Flip Service selector
              echo "Step 7/7: Flipping Service selector to $STANDBY"
              kubectl -n "$NAMESPACE" patch svc "$SERVICE" --type=json \
                -p "[{\"op\": \"replace\", \"path\": \"/spec/selector/app.kubernetes.io~1slot\", \"value\": \"$STANDBY\"}]"

              NEW_ACTIVE=$(kubectl -n "$NAMESPACE" get svc "$SERVICE" \
                -o jsonpath='{.spec.selector.app\.kubernetes\.io/slot}')
              echo "Service now pointing to: $NEW_ACTIVE"

              echo ""
              echo "=== Switchover complete ==="
              echo "Active: $STANDBY ($STANDBY_DEPLOY) running $NEW_IMAGE"
              echo "Standby: $ACTIVE ($ACTIVE_DEPLOY) scaled to 0"
          resources:
            requests:
              memory: 32Mi
              cpu: 10m
            limits:
              memory: 64Mi
{{- end }}
