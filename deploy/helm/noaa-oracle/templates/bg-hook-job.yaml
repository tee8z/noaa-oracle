{{- if .Values.blueGreen.enabled }}
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ include "noaa-oracle.fullname" . }}-bg-switchover
  labels:
    {{- include "noaa-oracle.labels" . | nindent 4 }}
  annotations:
    argocd.argoproj.io/hook: PreSync
    argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
spec:
  backoffLimit: 0
  activeDeadlineSeconds: 600
  ttlSecondsAfterFinished: 300
  template:
    metadata:
      labels:
        {{- include "noaa-oracle.selectorLabels" . | nindent 8 }}
        app.kubernetes.io/component: bg-switchover
    spec:
      serviceAccountName: {{ include "noaa-oracle.fullname" . }}-bg-hook
      restartPolicy: Never
      containers:
        - name: switchover
          image: bitnami/kubectl:latest
          command: ["/bin/bash", "-c"]
          args:
            - |
              set -euo pipefail

              NAMESPACE="{{ .Release.Namespace }}"
              SERVICE="{{ include "noaa-oracle.fullname" . }}"
              DEPLOY_PREFIX="{{ include "noaa-oracle.fullname" . }}"
              CONTAINER="{{ .Chart.Name }}"
              NEW_IMAGE="{{ .Values.image.repository }}:{{ .Values.image.tag }}"

              echo "=== Blue/Green PreSync Switchover ==="
              echo "Namespace: $NAMESPACE"
              echo "Service: $SERVICE"
              echo "New image: $NEW_IMAGE"

              # Get current active slot from Service selector
              ACTIVE=$(kubectl -n "$NAMESPACE" get svc "$SERVICE" \
                -o jsonpath='{.spec.selector.app\.kubernetes\.io/slot}' 2>/dev/null || echo "")

              if [[ -z "$ACTIVE" ]]; then
                echo "No slot selector found on Service. First deploy - initializing."
                echo "Scaling up ${DEPLOY_PREFIX}-blue..."
                kubectl -n "$NAMESPACE" scale deployment "${DEPLOY_PREFIX}-blue" --replicas=1 || true
                echo "Waiting for ${DEPLOY_PREFIX}-blue to be ready..."
                kubectl -n "$NAMESPACE" rollout status deployment "${DEPLOY_PREFIX}-blue" --timeout=300s || true
                echo "Patching Service selector to blue..."
                kubectl -n "$NAMESPACE" patch svc "$SERVICE" --type=json \
                  -p '[{"op": "add", "path": "/spec/selector/app.kubernetes.io~1slot", "value": "blue"}]' || true
                echo "Scaling down ${DEPLOY_PREFIX}-green..."
                kubectl -n "$NAMESPACE" scale deployment "${DEPLOY_PREFIX}-green" --replicas=0 || true
                echo "First deploy initialization complete."
                exit 0
              fi

              # Determine standby
              if [[ "$ACTIVE" == "blue" ]]; then STANDBY="green"; else STANDBY="blue"; fi

              ACTIVE_DEPLOY="${DEPLOY_PREFIX}-${ACTIVE}"
              STANDBY_DEPLOY="${DEPLOY_PREFIX}-${STANDBY}"

              echo "Active: $ACTIVE ($ACTIVE_DEPLOY)"
              echo "Standby: $STANDBY ($STANDBY_DEPLOY)"

              # Check if active is already running the new image
              CURRENT_IMAGE=$(kubectl -n "$NAMESPACE" get deployment "$ACTIVE_DEPLOY" \
                -o jsonpath="{.spec.template.spec.containers[?(@.name=='$CONTAINER')].image}" 2>/dev/null || echo "")
              echo "Current active image: $CURRENT_IMAGE"

              if [[ "$CURRENT_IMAGE" == "$NEW_IMAGE" ]]; then
                echo "Active deployment already running $NEW_IMAGE. No switchover needed."
                kubectl -n "$NAMESPACE" scale deployment "$STANDBY_DEPLOY" --replicas=0 2>/dev/null || true
                exit 0
              fi

              echo ""
              echo "=== Starting switchover: $ACTIVE -> $STANDBY ==="

              # Step 1: Update standby image
              echo "Step 1/8: Setting $STANDBY_DEPLOY image to $NEW_IMAGE"
              kubectl -n "$NAMESPACE" set image "deployment/$STANDBY_DEPLOY" "$CONTAINER=$NEW_IMAGE"

              # Step 2: Scale down active so Litestream flushes WAL to S3 on SIGTERM
              echo "Step 2/8: Scaling down $ACTIVE_DEPLOY (Litestream will flush WAL to S3)"
              kubectl -n "$NAMESPACE" scale deployment "$ACTIVE_DEPLOY" --replicas=0

              # Step 3: Wait for active pods to terminate
              echo "Step 3/8: Waiting for $ACTIVE_DEPLOY pods to terminate"
              kubectl -n "$NAMESPACE" wait --for=delete pod \
                -l "app.kubernetes.io/slot=$ACTIVE,app.kubernetes.io/name={{ include "noaa-oracle.name" . }}" \
                --timeout=60s 2>/dev/null || true
              echo "$ACTIVE_DEPLOY terminated, S3 backups should be up to date"

              # Step 4: Clean standby PVC so the litestream-restore init container
              # does a full restore from S3 instead of skipping (it uses -if-db-not-exists).
              # Without this, stale DB files on the standby PVC would be used as-is.
              echo "Step 4/8: Cleaning standby PVC (${DEPLOY_PREFIX}-${STANDBY})"
              CLEANUP_POD="pvc-cleanup-${STANDBY}-$(date +%s)"
              kubectl -n "$NAMESPACE" apply -f - <<CLEANUP_EOF
apiVersion: v1
kind: Pod
metadata:
  name: ${CLEANUP_POD}
spec:
  restartPolicy: Never
  containers:
    - name: cleanup
      image: alpine:latest
      command:
        - sh
        - -c
        - |
          echo 'Cleaning PVC files...'
          find {{ .Values.persistence.mountPath }} -type f -name '*.db' -exec rm -fv {} +
          find {{ .Values.persistence.mountPath }} -type f -name '*.db-shm' -exec rm -fv {} +
          find {{ .Values.persistence.mountPath }} -type f -name '*.db-wal' -exec rm -fv {} +
          find {{ .Values.persistence.mountPath }} -type f -name '*.sqlite' -exec rm -fv {} +
          find {{ .Values.persistence.mountPath }} -type f -name '*.sqlite-shm' -exec rm -fv {} +
          find {{ .Values.persistence.mountPath }} -type f -name '*.sqlite-wal' -exec rm -fv {} +
          find {{ .Values.persistence.mountPath }} -type d -name '*-litestream' -exec rm -rfv {} + 2>/dev/null
          echo 'PVC cleaned'
      volumeMounts:
        - name: data
          mountPath: {{ .Values.persistence.mountPath }}
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: ${DEPLOY_PREFIX}-${STANDBY}
CLEANUP_EOF
              echo "Waiting for cleanup pod to complete..."
              # Poll until pod reaches a terminal phase (Succeeded/Failed)
              for i in $(seq 1 60); do
                PHASE=$(kubectl -n "$NAMESPACE" get pod "$CLEANUP_POD" -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
                if [[ "$PHASE" == "Succeeded" || "$PHASE" == "Failed" ]]; then
                  echo "Cleanup pod finished with phase: $PHASE"
                  break
                fi
                sleep 1
              done
              kubectl -n "$NAMESPACE" logs "$CLEANUP_POD" 2>/dev/null || true
              kubectl -n "$NAMESPACE" delete pod "$CLEANUP_POD" --ignore-not-found 2>/dev/null || true

              # Step 5: Scale up standby (triggers Litestream restore from fresh S3 data)
              echo "Step 5/8: Scaling up $STANDBY_DEPLOY (will restore DB from S3)"
              kubectl -n "$NAMESPACE" scale deployment "$STANDBY_DEPLOY" --replicas=1

              # Step 6: Wait for readiness
              echo "Step 6/8: Waiting for $STANDBY_DEPLOY to be ready..."
              if ! kubectl -n "$NAMESPACE" rollout status deployment "$STANDBY_DEPLOY" --timeout=300s; then
                echo "ERROR: Standby deployment failed readiness check"
                echo "Rolling back: scaling down standby, scaling up previous active"
                kubectl -n "$NAMESPACE" scale deployment "$STANDBY_DEPLOY" --replicas=0
                kubectl -n "$NAMESPACE" scale deployment "$ACTIVE_DEPLOY" --replicas=1
                exit 1
              fi

              # Step 7: Flip Service selector
              echo "Step 7/8: Flipping Service selector to $STANDBY"
              kubectl -n "$NAMESPACE" patch svc "$SERVICE" --type=json \
                -p "[{\"op\": \"replace\", \"path\": \"/spec/selector/app.kubernetes.io~1slot\", \"value\": \"$STANDBY\"}]"

              NEW_ACTIVE=$(kubectl -n "$NAMESPACE" get svc "$SERVICE" \
                -o jsonpath='{.spec.selector.app\.kubernetes\.io/slot}')
              echo "Service now pointing to: $NEW_ACTIVE"


              # Step 8: Update activeSlot in infrastructure values via GitHub API.
              # This keeps the git values file in sync with the runtime state so
              # future Helm renders are correct. Uses the ghcr-pull-secret token.
              echo "Step 8/8: Updating activeSlot in infrastructure repo"
              GITHUB_TOKEN=""
              # Try to get token from ghcr-pull-secret in current namespace
              DOCKER_CONFIG=$(kubectl -n "$NAMESPACE" get secret ghcr-pull-secret -o jsonpath='{.data.\.dockerconfigjson}' 2>/dev/null || echo "")
              if [[ -n "$DOCKER_CONFIG" ]]; then
                GITHUB_TOKEN=$(echo "$DOCKER_CONFIG" | base64 -d | jq -r '.auths["ghcr.io"].auth' | base64 -d | cut -d: -f2)
              fi
              if [[ -z "$GITHUB_TOKEN" ]]; then
                echo "WARNING: Could not extract GitHub token from ghcr-pull-secret. Skipping git write-back."
              else
                VALUES_PATH="k8s/values/apps/staging/noaa-oracle.yaml"
                GITHUB_API="https://api.github.com/repos/5day4cast/infrastructure/contents/$VALUES_PATH"
                
                # Get current file content and SHA
                FILE_RESP=$(curl -sf -H "Authorization: token $GITHUB_TOKEN" -H "Accept: application/vnd.github.v3+json" "$GITHUB_API" 2>/dev/null)
                if [[ -n "$FILE_RESP" ]]; then
                  FILE_SHA=$(echo "$FILE_RESP" | jq -r '.sha')
                  FILE_CONTENT=$(echo "$FILE_RESP" | jq -r '.content' | base64 -d)
                  
                  # Update activeSlot value using sed
                  NEW_CONTENT=$(echo "$FILE_CONTENT" | sed "s/activeSlot: .*/activeSlot: $STANDBY/")
                  
                  # Only commit if the value actually changed
                  if [[ "$FILE_CONTENT" != "$NEW_CONTENT" ]]; then
                    NEW_CONTENT_B64=$(echo "$NEW_CONTENT" | base64 -w 0)
                    COMMIT_MSG="chore: update noaa-oracle activeSlot to $STANDBY after blue/green switchover"
                    
                    RESP=$(curl -sf -X PUT -H "Authorization: token $GITHUB_TOKEN" -H "Accept: application/vnd.github.v3+json" \
                      "$GITHUB_API" \
                      -d "$(jq -n --arg msg "$COMMIT_MSG" --arg content "$NEW_CONTENT_B64" --arg sha "$FILE_SHA" \
                        '{message: $msg, content: $content, sha: $sha}')" 2>/dev/null)
                    
                    if [[ -n "$RESP" ]]; then
                      COMMIT_SHA=$(echo "$RESP" | jq -r '.commit.sha // empty')
                      if [[ -n "$COMMIT_SHA" ]]; then
                        echo "Successfully updated activeSlot to $STANDBY (commit: $COMMIT_SHA)"
                      else
                        echo "WARNING: GitHub API response did not contain commit SHA. Write-back may have failed."
                      fi
                    else
                      echo "WARNING: GitHub API write-back failed. activeSlot in git may be stale."
                    fi
                  else
                    echo "activeSlot already set to $STANDBY in git. No update needed."
                  fi
                else
                  echo "WARNING: Could not fetch values file from GitHub API. Skipping git write-back."
                fi
              fi

              echo ""
              echo "=== Switchover complete ==="
              echo "Active: $STANDBY ($STANDBY_DEPLOY) running $NEW_IMAGE"
              echo "Standby: $ACTIVE ($ACTIVE_DEPLOY) scaled to 0"
          resources:
            requests:
              memory: 32Mi
              cpu: 10m
            limits:
              memory: 64Mi
{{- end }}
