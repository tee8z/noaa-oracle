{{- if .Values.blueGreen.enabled }}
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ include "noaa-oracle.fullname" . }}-bg-switchover
  labels:
    {{- include "noaa-oracle.labels" . | nindent 4 }}
  annotations:
    argocd.argoproj.io/hook: PreSync
    argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
spec:
  backoffLimit: 0
  activeDeadlineSeconds: 900
  ttlSecondsAfterFinished: 300
  template:
    metadata:
      labels:
        {{- include "noaa-oracle.selectorLabels" . | nindent 8 }}
        app.kubernetes.io/component: bg-switchover
    spec:
      serviceAccountName: {{ include "noaa-oracle.fullname" . }}-bg-hook
      restartPolicy: Never
      containers:
        - name: switchover
          image: bitnami/kubectl:latest
          command: ["/bin/bash", "-c"]
          args:
            - |
              set -euo pipefail

              NAMESPACE="{{ .Release.Namespace }}"
              SERVICE="{{ include "noaa-oracle.fullname" . }}"
              DEPLOY_PREFIX="{{ include "noaa-oracle.fullname" . }}"
              CONTAINER="{{ .Chart.Name }}"
              NEW_IMAGE="{{ .Values.image.repository }}:{{ .Values.image.tag }}"
              MOUNT_PATH="{{ .Values.persistence.mountPath }}"
              WEATHER_DIR="{{ .Values.config.dataDir }}"
              S3_BUCKET="{{ .Values.litestream.s3Bucket }}"
              S3_REGION="{{ .Values.litestream.s3Region }}"
              RETENTION_DAYS="{{ .Values.cleanup.retentionDays | default 32 }}"

              echo "=== Blue/Green PreSync Switchover ==="
              echo "Namespace: $NAMESPACE"
              echo "Service: $SERVICE"
              echo "New image: $NEW_IMAGE"

              # Get current active slot from Service selector
              ACTIVE=$(kubectl -n "$NAMESPACE" get svc "$SERVICE" \
                -o jsonpath='{.spec.selector.app\.kubernetes\.io/slot}' 2>/dev/null || echo "")

              if [[ -z "$ACTIVE" ]]; then
                echo "No slot selector found on Service. First deploy - initializing."
                echo "Scaling up ${DEPLOY_PREFIX}-blue..."
                kubectl -n "$NAMESPACE" scale deployment "${DEPLOY_PREFIX}-blue" --replicas=1 || true
                echo "Waiting for ${DEPLOY_PREFIX}-blue to be ready..."
                kubectl -n "$NAMESPACE" rollout status deployment "${DEPLOY_PREFIX}-blue" --timeout=300s || true
                echo "Patching Service selector to blue..."
                kubectl -n "$NAMESPACE" patch svc "$SERVICE" --type=json \
                  -p '[{"op": "add", "path": "/spec/selector/app.kubernetes.io~1slot", "value": "blue"}]' || true
                echo "Scaling down ${DEPLOY_PREFIX}-green..."
                kubectl -n "$NAMESPACE" scale deployment "${DEPLOY_PREFIX}-green" --replicas=0 || true
                echo "First deploy initialization complete."
                exit 0
              fi

              # Determine standby
              if [[ "$ACTIVE" == "blue" ]]; then STANDBY="green"; else STANDBY="blue"; fi

              ACTIVE_DEPLOY="${DEPLOY_PREFIX}-${ACTIVE}"
              STANDBY_DEPLOY="${DEPLOY_PREFIX}-${STANDBY}"

              echo "Active: $ACTIVE ($ACTIVE_DEPLOY)"
              echo "Standby: $STANDBY ($STANDBY_DEPLOY)"

              # Check if active is already running the new image
              CURRENT_IMAGE=$(kubectl -n "$NAMESPACE" get deployment "$ACTIVE_DEPLOY" \
                -o jsonpath="{.spec.template.spec.containers[?(@.name=='$CONTAINER')].image}" 2>/dev/null || echo "")
              echo "Current active image: $CURRENT_IMAGE"

              if [[ "$CURRENT_IMAGE" == "$NEW_IMAGE" ]]; then
                echo "Active deployment already running $NEW_IMAGE. No switchover needed."
                kubectl -n "$NAMESPACE" scale deployment "$STANDBY_DEPLOY" --replicas=0 2>/dev/null || true
                exit 0
              fi

              echo ""
              echo "=== Starting switchover: $ACTIVE -> $STANDBY ==="

              # Step 1: Update standby image
              echo "Step 1/13: Setting $STANDBY_DEPLOY image to $NEW_IMAGE"
              kubectl -n "$NAMESPACE" set image "deployment/$STANDBY_DEPLOY" "$CONTAINER=$NEW_IMAGE"

              # Step 2: Scale down active so Litestream flushes WAL to S3 on SIGTERM
              echo "Step 2/13: Scaling down $ACTIVE_DEPLOY (Litestream will flush WAL to S3)"
              kubectl -n "$NAMESPACE" scale deployment "$ACTIVE_DEPLOY" --replicas=0

              # Step 3: Wait for active pods to terminate
              echo "Step 3/13: Waiting for $ACTIVE_DEPLOY pods to terminate"
              kubectl -n "$NAMESPACE" wait --for=delete pod \
                -l "app.kubernetes.io/slot=$ACTIVE,app.kubernetes.io/name={{ include "noaa-oracle.name" . }}" \
                --timeout=60s 2>/dev/null || true
              echo "$ACTIVE_DEPLOY terminated"

              # Step 4: Wait for Litestream S3 sync to propagate.
              # The pod is gone but the S3 PUT may still be in-flight or eventually-consistent.
              echo "Step 4/13: Waiting for Litestream S3 sync to propagate"
              sleep 10
              echo "S3 sync propagation window complete"

              # Step 5: Clean standby PVC so the litestream-restore init container
              # does a full restore from S3 instead of skipping (it uses -if-db-not-exists).
              # Without this, stale DB files on the standby PVC would be used as-is.
              echo "Step 5/13: Cleaning standby PVC (${DEPLOY_PREFIX}-${STANDBY})"
              CLEANUP_POD="pvc-cleanup-${STANDBY}-$(date +%s)"
              CLEANUP_CMD="echo Cleaning PVC files...; find $MOUNT_PATH -type f -name '*.db' -exec rm -fv {} +; find $MOUNT_PATH -type f -name '*.db-shm' -exec rm -fv {} +; find $MOUNT_PATH -type f -name '*.db-wal' -exec rm -fv {} +; find $MOUNT_PATH -type f -name '*.sqlite' -exec rm -fv {} +; find $MOUNT_PATH -type f -name '*.sqlite-shm' -exec rm -fv {} +; find $MOUNT_PATH -type f -name '*.sqlite-wal' -exec rm -fv {} +; find $MOUNT_PATH -type d -name '*-litestream' -exec rm -rfv {} + 2>/dev/null; rm -fv $MOUNT_PATH/events/*.db $MOUNT_PATH/events/*.sqlite $MOUNT_PATH/events/*.sqlite-shm $MOUNT_PATH/events/*.sqlite-wal 2>/dev/null; rm -rfv $MOUNT_PATH/events/.*-litestream 2>/dev/null; echo PVC cleaned"
              # Create cleanup pod via JSON to avoid Helm YAML parser issues with heredocs
              printf '{"apiVersion":"v1","kind":"Pod","metadata":{"name":"%s"},"spec":{"restartPolicy":"Never","containers":[{"name":"cleanup","image":"alpine:latest","command":["sh","-c","%s"],"volumeMounts":[{"name":"data","mountPath":"%s"}]}],"volumes":[{"name":"data","persistentVolumeClaim":{"claimName":"%s"}}]}}' \
                "$CLEANUP_POD" "$CLEANUP_CMD" "$MOUNT_PATH" "${DEPLOY_PREFIX}-${STANDBY}" \
                | kubectl -n "$NAMESPACE" apply -f - 2>/dev/null || echo "WARNING: Failed to create cleanup pod"
              echo "Waiting for cleanup pod to complete..."
              # Poll until pod reaches a terminal phase (Succeeded/Failed)
              for i in $(seq 1 60); do
                PHASE=$(kubectl -n "$NAMESPACE" get pod "$CLEANUP_POD" -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
                if [[ "$PHASE" == "Succeeded" || "$PHASE" == "Failed" ]]; then
                  echo "Cleanup pod finished with phase: $PHASE"
                  break
                fi
                sleep 1
              done
              kubectl -n "$NAMESPACE" logs "$CLEANUP_POD" 2>/dev/null || true
              kubectl -n "$NAMESPACE" delete pod "$CLEANUP_POD" --ignore-not-found 2>/dev/null || true

              # Step 6: Verify standby PVC is clean. If stale DB files remain,
              # the litestream-restore init container will skip restore (-if-db-not-exists)
              # and the standby starts with old/corrupt data.
              echo "Step 6/13: Verifying standby PVC is clean"
              VERIFY_POD="pvc-verify-${STANDBY}-$(date +%s)"
              printf '{"apiVersion":"v1","kind":"Pod","metadata":{"name":"%s"},"spec":{"restartPolicy":"Never","containers":[{"name":"verify","image":"alpine:latest","command":["sh","-c","find %s -name '"'"'*.db'"'"' -o -name '"'"'*.sqlite'"'"' -o -name '"'"'*-litestream'"'"' 2>/dev/null | wc -l"],"volumeMounts":[{"name":"data","mountPath":"%s"}]}],"volumes":[{"name":"data","persistentVolumeClaim":{"claimName":"%s"}}]}}' \
                "$VERIFY_POD" "$MOUNT_PATH" "$MOUNT_PATH" "${DEPLOY_PREFIX}-${STANDBY}" \
                | kubectl -n "$NAMESPACE" apply -f - 2>/dev/null || echo "WARNING: Failed to create verify pod"
              for i in $(seq 1 30); do
                PHASE=$(kubectl -n "$NAMESPACE" get pod "$VERIFY_POD" -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
                if [[ "$PHASE" == "Succeeded" || "$PHASE" == "Failed" ]]; then break; fi
                sleep 1
              done
              STALE_FILES=$(kubectl -n "$NAMESPACE" logs "$VERIFY_POD" 2>/dev/null | tr -d '[:space:]')
              kubectl -n "$NAMESPACE" delete pod "$VERIFY_POD" --ignore-not-found 2>/dev/null || true
              if [[ -n "$STALE_FILES" && "$STALE_FILES" -gt 0 ]] 2>/dev/null; then
                echo "ERROR: PVC cleanup failed: $STALE_FILES database files still present on ${DEPLOY_PREFIX}-${STANDBY}"
                echo "Rolling back: scaling up previous active"
                kubectl -n "$NAMESPACE" scale deployment "$ACTIVE_DEPLOY" --replicas=1
                exit 1
              fi
              echo "Standby PVC is clean, restore will fetch fresh data from S3"

              # Step 7: Scale up standby (triggers Litestream restore from fresh S3 data)
              echo "Step 7/13: Scaling up $STANDBY_DEPLOY (will restore DB from S3)"
              kubectl -n "$NAMESPACE" scale deployment "$STANDBY_DEPLOY" --replicas=1

              # Step 8: Wait for readiness
              echo "Step 8/13: Waiting for $STANDBY_DEPLOY to be ready..."
              if ! kubectl -n "$NAMESPACE" rollout status deployment "$STANDBY_DEPLOY" --timeout=300s; then
                echo "ERROR: Standby deployment failed readiness check"
                echo "Rolling back: scaling down standby, scaling up previous active"
                kubectl -n "$NAMESPACE" scale deployment "$STANDBY_DEPLOY" --replicas=0
                kubectl -n "$NAMESPACE" scale deployment "$ACTIVE_DEPLOY" --replicas=1
                exit 1
              fi

              # Step 9: Verify restored database integrity before routing traffic.
              # Uses a temporary pod pinned to the standby node for RWO PVC access.
              echo "Step 9/13: Verifying database integrity on standby"
              STANDBY_NODE=$(kubectl -n "$NAMESPACE" get pod \
                -l "app.kubernetes.io/slot=$STANDBY,app.kubernetes.io/name={{ include "noaa-oracle.name" . }}" \
                -o jsonpath='{.items[0].spec.nodeName}' 2>/dev/null)
              INTEGRITY_POD="db-integrity-${STANDBY}-$(date +%s)"
              INTEGRITY_CMD="apk add --no-cache sqlite >/dev/null 2>&1 && find $MOUNT_PATH -name '*.db' -o -name '*.sqlite' | while read db; do result=\$(sqlite3 \"\$db\" 'PRAGMA quick_check;' 2>&1); if [ \"\$result\" != 'ok' ]; then echo \"FAILED:\$db:\$result\"; fi; done; echo DONE"
              printf '{"apiVersion":"v1","kind":"Pod","metadata":{"name":"%s"},"spec":{"nodeName":"%s","restartPolicy":"Never","containers":[{"name":"check","image":"alpine:latest","command":["sh","-c","%s"],"volumeMounts":[{"name":"data","mountPath":"%s"}]}],"volumes":[{"name":"data","persistentVolumeClaim":{"claimName":"%s"}}]}}' \
                "$INTEGRITY_POD" "$STANDBY_NODE" "$INTEGRITY_CMD" "$MOUNT_PATH" "${DEPLOY_PREFIX}-${STANDBY}" \
                | kubectl -n "$NAMESPACE" apply -f - 2>/dev/null || echo "WARNING: Failed to create integrity check pod"
              for i in $(seq 1 60); do
                PHASE=$(kubectl -n "$NAMESPACE" get pod "$INTEGRITY_POD" -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
                if [[ "$PHASE" == "Succeeded" || "$PHASE" == "Failed" ]]; then break; fi
                sleep 1
              done
              DB_INTEGRITY=$(kubectl -n "$NAMESPACE" logs "$INTEGRITY_POD" 2>/dev/null)
              kubectl -n "$NAMESPACE" delete pod "$INTEGRITY_POD" --ignore-not-found 2>/dev/null || true

              if echo "$DB_INTEGRITY" | grep -q "FAILED:"; then
                echo "WARNING: Integrity check failed, attempting REINDEX on affected databases..."

                # Scale down standby for exclusive PVC access during repair
                kubectl -n "$NAMESPACE" scale deployment "$STANDBY_DEPLOY" --replicas=0
                kubectl -n "$NAMESPACE" wait --for=delete pod \
                  -l "app.kubernetes.io/slot=$STANDBY,app.kubernetes.io/name={{ include "noaa-oracle.name" . }}" \
                  --timeout=60s 2>/dev/null || true

                REPAIR_POD="db-repair-${STANDBY}-$(date +%s)"
                REPAIR_CMD="apk add --no-cache sqlite >/dev/null 2>&1 && find $MOUNT_PATH -name '*.db' -o -name '*.sqlite' | while read db; do sqlite3 \"\$db\" 'REINDEX; VACUUM;' 2>&1; result=\$(sqlite3 \"\$db\" 'PRAGMA integrity_check;' 2>&1); if [ \"\$result\" != 'ok' ]; then echo \"FAILED:\$db:\$result\"; fi; done; echo DONE"
                printf '{"apiVersion":"v1","kind":"Pod","metadata":{"name":"%s"},"spec":{"nodeName":"%s","restartPolicy":"Never","containers":[{"name":"repair","image":"alpine:latest","command":["sh","-c","%s"],"volumeMounts":[{"name":"data","mountPath":"%s"}]}],"volumes":[{"name":"data","persistentVolumeClaim":{"claimName":"%s"}}]}}' \
                  "$REPAIR_POD" "$STANDBY_NODE" "$REPAIR_CMD" "$MOUNT_PATH" "${DEPLOY_PREFIX}-${STANDBY}" \
                  | kubectl -n "$NAMESPACE" apply -f - 2>/dev/null || true
                for i in $(seq 1 60); do
                  PHASE=$(kubectl -n "$NAMESPACE" get pod "$REPAIR_POD" -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
                  if [[ "$PHASE" == "Succeeded" || "$PHASE" == "Failed" ]]; then break; fi
                  sleep 1
                done
                REPAIR_RESULT=$(kubectl -n "$NAMESPACE" logs "$REPAIR_POD" 2>/dev/null)
                kubectl -n "$NAMESPACE" delete pod "$REPAIR_POD" --ignore-not-found 2>/dev/null || true

                if echo "$REPAIR_RESULT" | grep -q "FAILED:"; then
                  echo "ERROR: Database integrity check failed after REINDEX"
                  echo "$REPAIR_RESULT" | grep "FAILED:"
                  echo "Rolling back: scaling up previous active"
                  kubectl -n "$NAMESPACE" scale deployment "$ACTIVE_DEPLOY" --replicas=1
                  exit 1
                fi
                echo "REINDEX repaired integrity issues, scaling standby back up"
                kubectl -n "$NAMESPACE" scale deployment "$STANDBY_DEPLOY" --replicas=1
                if ! kubectl -n "$NAMESPACE" rollout status deployment "$STANDBY_DEPLOY" --timeout=300s; then
                  echo "ERROR: Standby failed to restart after REINDEX, rolling back"
                  kubectl -n "$NAMESPACE" scale deployment "$STANDBY_DEPLOY" --replicas=0
                  kubectl -n "$NAMESPACE" scale deployment "$ACTIVE_DEPLOY" --replicas=1
                  exit 1
                fi
              else
                echo "Database integrity check passed"
              fi


              # Step 10/13: Restore weather parquet data from S3 if standby PVC is empty.
              # On normal switchovers the weather dir survives Step 5 (which only deletes
              # DB files). But on a fresh/recreated PVC, the weather dir is empty and only
              # the SQLite DB gets restored via Litestream.
              echo "Step 10/13: Checking weather data on standby PVC"
              WEATHER_CHECK_POD="weather-check-${STANDBY}-$(date +%s)"
              printf '{"apiVersion":"v1","kind":"Pod","metadata":{"name":"%s"},"spec":{"nodeName":"%s","restartPolicy":"Never","containers":[{"name":"check","image":"alpine:latest","command":["sh","-c","find %s -name '"'"'*.parquet'"'"' 2>/dev/null | wc -l"],"volumeMounts":[{"name":"data","mountPath":"%s"}]}],"volumes":[{"name":"data","persistentVolumeClaim":{"claimName":"%s"}}]}}' \
                "$WEATHER_CHECK_POD" "$STANDBY_NODE" "$WEATHER_DIR" "$MOUNT_PATH" "${DEPLOY_PREFIX}-${STANDBY}" \
                | kubectl -n "$NAMESPACE" apply -f - 2>/dev/null || echo "WARNING: Failed to create weather check pod"
              for i in $(seq 1 30); do
                PHASE=$(kubectl -n "$NAMESPACE" get pod "$WEATHER_CHECK_POD" -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
                if [[ "$PHASE" == "Succeeded" || "$PHASE" == "Failed" ]]; then break; fi
                sleep 1
              done
              PARQUET_COUNT=$(kubectl -n "$NAMESPACE" logs "$WEATHER_CHECK_POD" 2>/dev/null | tr -d '[:space:]')
              kubectl -n "$NAMESPACE" delete pod "$WEATHER_CHECK_POD" --ignore-not-found 2>/dev/null || true

              if [[ -n "$PARQUET_COUNT" && "$PARQUET_COUNT" -gt 0 ]] 2>/dev/null; then
                echo "Weather data present ($PARQUET_COUNT parquet files). Skipping S3 restore."
              else
                echo "No weather data found on standby PVC. Restoring from S3..."

                # Scale down standby to release RWO PVC for the restore pod
                kubectl -n "$NAMESPACE" scale deployment "$STANDBY_DEPLOY" --replicas=0
                kubectl -n "$NAMESPACE" wait --for=delete pod \
                  -l "app.kubernetes.io/slot=$STANDBY,app.kubernetes.io/name={{ include "noaa-oracle.name" . }}" \
                  --timeout=60s 2>/dev/null || true

                RESTORE_POD="weather-restore-${STANDBY}-$(date +%s)"
                RESTORE_CMD="mkdir -p ${WEATHER_DIR} && aws s3 sync s3://${S3_BUCKET}/weather_data/ ${WEATHER_DIR}/ --region ${S3_REGION} --only-show-errors && TOTAL=\$(find ${WEATHER_DIR} -name '*.parquet' 2>/dev/null | wc -l) && echo \"Restored \$TOTAL parquet files\""
                printf '{"apiVersion":"v1","kind":"Pod","metadata":{"name":"%s"},"spec":{"nodeName":"%s","restartPolicy":"Never","containers":[{"name":"restore","image":"amazon/aws-cli:latest","command":["sh","-c","%s"],"volumeMounts":[{"name":"data","mountPath":"%s"}]}],"volumes":[{"name":"data","persistentVolumeClaim":{"claimName":"%s"}}]}}' \
                  "$RESTORE_POD" "$STANDBY_NODE" "$RESTORE_CMD" "$MOUNT_PATH" "${DEPLOY_PREFIX}-${STANDBY}" \
                  | kubectl -n "$NAMESPACE" apply -f - 2>/dev/null || echo "WARNING: Failed to create weather restore pod"

                RESTORE_OK=false
                for i in $(seq 1 300); do
                  PHASE=$(kubectl -n "$NAMESPACE" get pod "$RESTORE_POD" -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
                  if [[ "$PHASE" == "Succeeded" ]]; then RESTORE_OK=true; break; fi
                  if [[ "$PHASE" == "Failed" ]]; then break; fi
                  sleep 1
                done
                kubectl -n "$NAMESPACE" logs "$RESTORE_POD" 2>/dev/null || true
                kubectl -n "$NAMESPACE" delete pod "$RESTORE_POD" --ignore-not-found 2>/dev/null || true

                if [[ "$RESTORE_OK" == "true" ]]; then
                  echo "Weather data restore from S3 completed successfully"
                else
                  echo "WARNING: Weather data restore failed. Run 'just restore-oracle-data' manually after switchover."
                fi

                # Scale standby back up
                kubectl -n "$NAMESPACE" scale deployment "$STANDBY_DEPLOY" --replicas=1
                if ! kubectl -n "$NAMESPACE" rollout status deployment "$STANDBY_DEPLOY" --timeout=300s; then
                  echo "ERROR: Standby failed to restart after weather restore, rolling back"
                  kubectl -n "$NAMESPACE" scale deployment "$STANDBY_DEPLOY" --replicas=0
                  kubectl -n "$NAMESPACE" scale deployment "$ACTIVE_DEPLOY" --replicas=1
                  exit 1
                fi
              fi

              # Step 11: Flip Service selector
              echo "Step 11/13: Flipping Service selector to $STANDBY"
              kubectl -n "$NAMESPACE" patch svc "$SERVICE" --type=json \
                -p "[{\"op\": \"replace\", \"path\": \"/spec/selector/app.kubernetes.io~1slot\", \"value\": \"$STANDBY\"}]"

              NEW_ACTIVE=$(kubectl -n "$NAMESPACE" get svc "$SERVICE" \
                -o jsonpath='{.spec.selector.app\.kubernetes\.io/slot}')
              echo "Service now pointing to: $NEW_ACTIVE"

              # Step 12: Scale down old active
              echo "Step 12/13: Scaling down old active $ACTIVE_DEPLOY"
              kubectl -n "$NAMESPACE" scale deployment "$ACTIVE_DEPLOY" --replicas=0 2>/dev/null || true

              # Step 13: Update activeSlot in infrastructure values via GitHub API.
              # This keeps the git values file in sync with the runtime state so
              # future Helm renders are correct. Uses the ghcr-pull-secret token.
              echo "Step 13/13: Updating activeSlot in infrastructure repo"
              GITHUB_TOKEN=""
              # Try to get token from ghcr-pull-secret in current namespace
              DOCKER_CONFIG=$(kubectl -n "$NAMESPACE" get secret ghcr-pull-secret -o jsonpath='{.data.\.dockerconfigjson}' 2>/dev/null || echo "")
              if [[ -n "$DOCKER_CONFIG" ]]; then
                GITHUB_TOKEN=$(echo "$DOCKER_CONFIG" | base64 -d | jq -r '.auths["ghcr.io"].auth' | base64 -d | cut -d: -f2)
              fi
              if [[ -z "$GITHUB_TOKEN" ]]; then
                echo "WARNING: Could not extract GitHub token from ghcr-pull-secret. Skipping git write-back."
              else
                VALUES_PATH="k8s/values/apps/staging/noaa-oracle.yaml"
                GITHUB_API="https://api.github.com/repos/5day4cast/infrastructure/contents/$VALUES_PATH"

                # Get current file content and SHA
                FILE_RESP=$(curl -sf -H "Authorization: token $GITHUB_TOKEN" -H "Accept: application/vnd.github.v3+json" "$GITHUB_API" 2>/dev/null)
                if [[ -n "$FILE_RESP" ]]; then
                  FILE_SHA=$(echo "$FILE_RESP" | jq -r '.sha')
                  FILE_CONTENT=$(echo "$FILE_RESP" | jq -r '.content' | base64 -d)

                  # Update activeSlot value using sed
                  NEW_CONTENT=$(echo "$FILE_CONTENT" | sed "s/activeSlot: .*/activeSlot: $STANDBY/")

                  # Only commit if the value actually changed
                  if [[ "$FILE_CONTENT" != "$NEW_CONTENT" ]]; then
                    NEW_CONTENT_B64=$(echo "$NEW_CONTENT" | base64 -w 0)
                    COMMIT_MSG="chore: update noaa-oracle activeSlot to $STANDBY after blue/green switchover"

                    RESP=$(curl -sf -X PUT -H "Authorization: token $GITHUB_TOKEN" -H "Accept: application/vnd.github.v3+json" \
                      "$GITHUB_API" \
                      -d "$(jq -n --arg msg "$COMMIT_MSG" --arg content "$NEW_CONTENT_B64" --arg sha "$FILE_SHA" \
                        '{message: $msg, content: $content, sha: $sha}')" 2>/dev/null)

                    if [[ -n "$RESP" ]]; then
                      COMMIT_SHA=$(echo "$RESP" | jq -r '.commit.sha // empty')
                      if [[ -n "$COMMIT_SHA" ]]; then
                        echo "Successfully updated activeSlot to $STANDBY (commit: $COMMIT_SHA)"
                      else
                        echo "WARNING: GitHub API response did not contain commit SHA. Write-back may have failed."
                      fi
                    else
                      echo "WARNING: GitHub API write-back failed. activeSlot in git may be stale."
                    fi
                  else
                    echo "activeSlot already set to $STANDBY in git. No update needed."
                  fi
                else
                  echo "WARNING: Could not fetch values file from GitHub API. Skipping git write-back."
                fi
              fi

              echo ""
              echo "=== Switchover complete ==="
              echo "Active: $STANDBY ($STANDBY_DEPLOY) running $NEW_IMAGE"
              echo "Standby: $ACTIVE ($ACTIVE_DEPLOY) scaled to 0"
          resources:
            requests:
              memory: 32Mi
              cpu: 10m
            limits:
              memory: 64Mi
{{- end }}
